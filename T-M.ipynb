{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4JvsZlUYy4tR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 02:51:02.656310: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-02 02:51:02.674838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-02 02:51:02.674866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-02 02:51:02.675527: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-02 02:51:02.679786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-02 02:51:03.096848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否可见GPU： [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 02:51:03.492598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.510603: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.510641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.510645: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "import os, json,time, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from operator import truediv\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA,  PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import (Conv3D, Conv2D, Conv1D, MaxPooling1D, Dense, Dropout,\n",
    "                          Flatten, Input, Reshape, MaxPooling2D, SeparableConv2D,\n",
    "                          MaxPooling3D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "                          BatchNormalization, GlobalAveragePooling3D, concatenate, Reshape)\n",
    "print(\"是否可见GPU：\", tf.config.list_physical_devices('GPU'))\n",
    "# import torch\n",
    "# print(\"PyTorch 是否可用 GPU：\", torch.cuda.is_available())\n",
    "# print(\"GPU 名称：\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"无\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In case of multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 02:51:03.517564: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.517633: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.517653: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.517657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2348] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2025-08-02 02:51:03.610254: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.610299: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.610304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-08-02 02:51:03.610326: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-02 02:51:03.610346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9131 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070, pci bus id: 0000:01:00.0, compute capability: 12.0\n"
     ]
    }
   ],
   "source": [
    "def set_gpu_device(device_index):\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_visible_devices(gpus[device_index], \"GPU\")\n",
    "            tf.config.experimental.set_memory_growth(gpus[device_index], True)\n",
    "            logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "# Call this function before any TensorFlow operations\n",
    "set_gpu_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Hyperspectral Datasets\n",
    "def LoadHSIData(method):\n",
    "    data_path = os.path.join(os.getcwd(),'./HSI/')\n",
    "    if method == 'HH':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HongHu.mat'))['WHU_Hi_HongHu']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HongHu_gt'))['WHU_Hi_HongHu_gt']\n",
    "        Num_Classes = 22\n",
    "        target_names = ['Red roof', 'Road', 'Bare soil', 'Cotton',\n",
    "                        'Cotton firewood', 'Rape', 'Chinese cabbage',\n",
    "                        'Pakchoi', 'Cabbage', 'Tuber mustard', 'Brassica parachinensis',\n",
    "                        'Brassica chinensis', 'Small Brassica chinensis', 'Lactuca sativa',\n",
    "                        'Celtuce', 'Film covered lettuce', 'Romaine lettuce',\n",
    "                        'Carrot', 'White radish', 'Garlic sprout', 'Broad bean',\n",
    "                        'Tree']\n",
    "    elif method == 'HC':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HanChuan.mat'))['WHU_Hi_HanChuan']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'WHU_Hi_HanChuan_gt'))['WHU_Hi_HanChuan_gt']\n",
    "        Num_Classes = 16\n",
    "        target_names = ['Strawberry', 'Cowpea', 'Soybean', 'Sorghum',\n",
    "                        'Water spinach', 'Watermelon', 'Greens', 'Trees', 'Grass',\n",
    "                        'Red roof', 'Gray roof', 'Plastic', 'Bare soil', 'Road',\n",
    "                        'Bright object', 'Water']\n",
    "    elif method == 'Qingyun':\n",
    "      HSI = sio.loadmat(os.path.join(data_path, 'QUH-Qingyun.mat'))['Chengqu']\n",
    "      GT = sio.loadmat(os.path.join(data_path, 'QUH-Qingyun_GT.mat'))['ChengquGT']\n",
    "      Num_Classes = 6\n",
    "      target_names = [\"Trees\", \"Concrete building\", \"Car\", \"Ironhide building\",\n",
    "                      \"Plastic playground\", \"Asphalt road\"]\n",
    "    elif method == 'SA':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "        Num_Classes = 16\n",
    "        target_names = ['Weeds_1','Weeds_2','Fallow',\n",
    "                        'Fallow_rough_plow','Fallow_smooth', 'Stubble','Celery',\n",
    "                        'Grapes_untrained','Soil_vinyard_develop','Corn_Weeds',\n",
    "                        'Lettuce_4wk','Lettuce_5wk','Lettuce_6wk',\n",
    "                        'Lettuce_7wk', 'Vinyard_untrained','Vinyard_trellis']\n",
    "    elif method == 'PU':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
    "        Num_Classes = 9\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted','Soil','Bitumen',\n",
    "                        'Bricks','Shadows']\n",
    "    elif method == 'UH':\n",
    "      HSI = sio.loadmat(os.path.join(data_path, 'HU.mat'))['HSI']\n",
    "      GT = sio.loadmat(os.path.join(data_path, 'HU_gt.mat'))['gt']\n",
    "      Num_Classes = 15\n",
    "      target_names = ['Healthy grass', 'Stressed grass', 'Synthetic grass', 'Trees',\n",
    "                    'Soil', 'Water', 'Residential', 'Commercial', 'Road',\n",
    "                    'Highway', 'Railway', 'Parking Lot 1', 'Parking Lot 2',\n",
    "                    'Tennis Court', 'Running Track']\n",
    "    return HSI, GT, Num_Classes, target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensional Reduction Method (If required)\n",
    "def DLMethod(method, HSI, NC = 75):\n",
    "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
    "    if method == 'PCA': ## PCA\n",
    "        pca = PCA(n_components = NC, whiten = True)\n",
    "        RHSI = pca.fit_transform(RHSI)\n",
    "        RHSI = np.reshape(RHSI, (HSI.shape[0], HSI.shape[1], NC))\n",
    "    elif method == 'iPCA': ## Incremental PCA\n",
    "        n_batches = 256\n",
    "        inc_pca = IncrementalPCA(n_components = NC)\n",
    "        for X_batch in np.array_split(RHSI, n_batches):\n",
    "          inc_pca.partial_fit(X_batch)\n",
    "        X_ipca = inc_pca.transform(RHSI)\n",
    "        RHSI = np.reshape(X_ipca, (HSI.shape[0], HSI.shape[1], NC))\n",
    "    return RHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrTeSplit(HSI, GT, trRatio, vrRatio, teRatio, randomState=345):\n",
    "    # Split into train and test sets\n",
    "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=teRatio,\n",
    "                                        random_state=randomState, stratify=GT)\n",
    "    # Calculate the validation ratio based on the updated test and train ratios\n",
    "    totalTrRatio = trRatio + vrRatio\n",
    "    new_vrRatio = vrRatio / totalTrRatio\n",
    "    # Split train set into train and validation sets\n",
    "    Tr, Va, TrC, VaC = train_test_split(Tr, TrC, test_size=new_vrRatio,\n",
    "                                        random_state=randomState, stratify=TrC)\n",
    "\n",
    "    return Tr, Va, Te, TrC, VaC, TeC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSID = \"Qingyun\" ## \"LK\", \"HC\", \"HH\", Qingyun, Pingan, Tangdaowan\n",
    "DLM = \"PCA\" ## \"PCA\", \"iPCA\"\n",
    "WS = 8\n",
    "teRatio = 0.50\n",
    "vrRatio = 0.90\n",
    "trRatio = 0.10\n",
    "k = 5\n",
    "adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.001, decay = 1e-06)\n",
    "epochs = 5\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creat Patches for 3D (Spatial-Spectral) Models\n",
    "def ImageCubes(HSI, GT, WS=WS, removeZeroLabels=True):\n",
    "    num_rows, num_cols, num_bands = HSI.shape\n",
    "    margin = int(WS / 2)\n",
    "    padded_data = np.pad(HSI, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n",
    "    image_cubes = np.zeros((num_rows * num_cols, WS, WS, num_bands))\n",
    "    patchesLabels = np.zeros((num_rows * num_cols))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, num_rows + margin):\n",
    "        for c in range(margin, num_cols + margin):\n",
    "            cube = padded_data[r - margin: r + margin, c - margin: c + margin, :]\n",
    "            image_cubes[patchIndex, :, :, :] = cube\n",
    "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "      image_cubes = image_cubes[patchesLabels>0,:,:,:]\n",
    "      patchesLabels = patchesLabels[patchesLabels>0]\n",
    "      patchesLabels -= 1\n",
    "    return image_cubes, patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning Class Labels for Final Classification and Confusion Matrices\n",
    "def ClassificationReports(TeC, Te_Pred, target_names):\n",
    "    classification = classification_report(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1), target_names = target_names)\n",
    "    oa = accuracy_score(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    confusion = confusion_matrix(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    list_diag = np.diag(confusion)\n",
    "    list_raw_sum = np.sum(confusion, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    aa = np.mean(each_acc)\n",
    "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100\n",
    "\n",
    "## Writing Results in CSV files\n",
    "def CSVResults(file_name, classification, Confusion, Parameters, \n",
    "                          Flops, Tr_time, Te_Time, Kappa, OA, AA, Per_Class):\n",
    "    classification = str(classification)\n",
    "    confusion = str(Confusion)\n",
    "    with open(file_name, 'w') as CSV_file:\n",
    "      CSV_file.write('{} Tr_Time'.format(Tr_time))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Te_Time'.format(Te_Time))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Flops'.format(Flops))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Parameters'.format(Parameters))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Kappa accuracy (%)'.format(Kappa))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Overall accuracy (%)'.format(OA))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Average accuracy (%)'.format(AA))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(Per_Class))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(classification))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(Confusion))\n",
    "    return CSV_file\n",
    "\n",
    "## Plot Ground Truths\n",
    "def GT_Plot(CRDHSI, GT, model, WS, k):\n",
    "  Predicted = model.predict(CRDHSI)\n",
    "  Predicted = np.argmax(Predicted, axis=1)\n",
    "  height, width = np.shape(GT)\n",
    "  ## Calculate the predicted Ground Truths\n",
    "  outputs = np.zeros((height, width))\n",
    "  count = 0\n",
    "  for AA in range(height):\n",
    "    for BB in range(width):\n",
    "      target = int(GT[AA,BB])\n",
    "      if target == 0:\n",
    "        continue\n",
    "      else:\n",
    "        outputs[AA][BB] = Predicted[count]\n",
    "        count = count+1\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Function to load Datasets, Dimensional Reduction and Creating Patchs\n",
    "HSI, GT, Num_Classes, target_names = LoadHSIData(HSID)\n",
    "start = time.time()\n",
    "RDHSI = DLMethod(DLM, HSI, NC = k)\n",
    "end = time.time()\n",
    "DL_Time = end - start\n",
    "CRDHSI, CGT = ImageCubes(RDHSI, GT, WS = WS)\n",
    "Tr, Va, Te, TrC, VaC, TeC = TrTeSplit(CRDHSI, CGT, trRatio, vrRatio, teRatio)\n",
    "TrC = to_categorical(TrC)\n",
    "VaC = to_categorical(VaC)\n",
    "TeC = to_categorical(TeC)\n",
    "CHSI = to_categorical(CGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Flops\n",
    "def get_flops(model):\n",
    "    total_flops = 0\n",
    "    for layer in model.layers:\n",
    "        flops = 0\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            kernel_size = layer.kernel_size[0] * layer.kernel_size[1] * layer.input_shape[-1]\n",
    "            flops = kernel_size * layer.output_shape[1] * layer.output_shape[2] * layer.filters\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            flops = layer.input_shape[-1] * layer.units\n",
    "        elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "            continue\n",
    "        total_flops += flops\n",
    "    return total_flops\n",
    "\n",
    "def compute_flops(model):\n",
    "    flops = get_flops(model)\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "def count_class_samples(labels, num_classes):\n",
    "    class_counts = {f\"class_{i}\": 0 for i in range(num_classes)}\n",
    "    for label in labels:\n",
    "        class_index = np.argmax(label)\n",
    "        class_counts[f\"class_{class_index}\"] += 1\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encodding \n",
    "def get_positional_encoding_ssvit(max_len, d_emb):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_emb)[np.newaxis, :]\n",
    "    angles = pos / np.power(10000, 2 * i / d_emb)\n",
    "    positional_encoding = np.zeros((max_len, d_emb))\n",
    "    positional_encoding[:, ::2] = np.sin(angles[:, ::2])\n",
    "    positional_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return positional_encoding[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class tokens\n",
    "class ClassTokenLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(ClassTokenLayer, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cls_token = self.add_weight(\n",
    "            name=\"cls_token\",\n",
    "            shape=(1, 1, 1, self.embedding_dim),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        cls_token_broadcasted = tf.broadcast_to(self.cls_token, [tf.shape(inputs)[0], 1, 1, 1, self.embedding_dim])\n",
    "        return tf.concat([cls_token_broadcasted, inputs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Mamba Block（模拟版）实现\n",
    "class MambaBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, **kwargs):\n",
    "        super(MambaBlock, self).__init__(**kwargs)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        # 模拟状态空间建模的卷积门控机制\n",
    "        self.state_update = tf.keras.layers.Conv1D(d_model, kernel_size=3, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        state = self.state_update(x)\n",
    "        x = x * state  # 状态门控\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Mamba 模型（替代原 SSViT）\n",
    "def MambaViT(WS, k, Num_Classes, num_mamba_layers=4, d_model=64, d_ff=256):\n",
    "    input_shape = (WS, WS, k)\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"inputs\")\n",
    "\n",
    "    # Patch embedding via Conv3D\n",
    "    x = tf.expand_dims(inputs, axis=1)  # [B, 1, H, W, C]\n",
    "    patch_embed = tf.keras.layers.Conv3D(\n",
    "        filters=d_model,\n",
    "        kernel_size=(1, WS, WS),\n",
    "        strides=(1, WS, WS),\n",
    "        padding='valid',\n",
    "        name=\"patch_embedding\"\n",
    "    )(x)\n",
    "\n",
    "    # Flatten patches to token sequence\n",
    "    B, T, H, W, C = patch_embed.shape\n",
    "    x = tf.reshape(patch_embed, (-1, T, C))  # [B, num_patches, d_model]\n",
    "\n",
    "    # 堆叠 Mamba Block\n",
    "    for i in range(num_mamba_layers):\n",
    "        x = MambaBlock(d_model=d_model, d_ff=d_ff, name=f\"mamba_block_{i}\")(x)\n",
    "\n",
    "    # 分类 token：平均池化\n",
    "    x = tf.reduce_mean(x, axis=1)  # [B, d_model]\n",
    "    x = tf.keras.layers.Dense(d_ff, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(d_ff, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(Num_Classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=\"MambaViT\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Sampling\n",
    "def uncertainty_sampling(model, X_pool, query_size):\n",
    "    probs = model.predict(X_pool)\n",
    "    uncertainty = -np.max(probs, axis=1)  # Lower max prob indicates higher uncertainty\n",
    "    query_indices = np.argsort(uncertainty)[-query_size:]  # Get indices of most uncertain samples\n",
    "    return query_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity Sampling \n",
    "def diversity_sampling(X_pool, query_size):\n",
    "    num_samples, H, W, C = X_pool.shape\n",
    "    query_indices = []\n",
    "    neighborhood_size=3\n",
    "    half_size = neighborhood_size // 2\n",
    "    for i in range(num_samples):\n",
    "        current_sample = X_pool[i]\n",
    "        diversity_values = np.zeros((H, W))\n",
    "        for h in range(half_size, H - half_size):\n",
    "            for w in range(half_size, W - half_size):\n",
    "                neighborhood = current_sample[h - half_size:h + half_size + 1, w - half_size:w + half_size + 1, :]\n",
    "                reshaped_neighborhood = neighborhood.reshape(-1, C)\n",
    "                distances = cdist(reshaped_neighborhood, reshaped_neighborhood, metric='euclidean')\n",
    "                diversity_metric = np.mean(distances)\n",
    "                diversity_values[h, w] = diversity_metric\n",
    "        flat_diversity_values = diversity_values.flatten()\n",
    "        query_indices = np.argsort(flat_diversity_values)[-query_size:]\n",
    "    return query_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Query\n",
    "def hybrid_query_strategy(model, X_pool, query_size):\n",
    "    uncertainty_indices = uncertainty_sampling(model, X_pool, query_size)\n",
    "    diversity_indices = diversity_sampling(X_pool, query_size)\n",
    "    query_indices = np.unique(np.concatenate((uncertainty_indices, diversity_indices)))\n",
    "    return query_indices[:query_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 02:51:15.990211: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2025-08-02 02:51:16.049174: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-02 02:51:16.052351: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 12.0\n",
      "2025-08-02 02:51:16.052386: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at /home/zhongruima/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc/bin/ptxas\n",
      "2025-08-02 02:51:16.052433: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052541: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052576: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052602: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052621: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052645: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.052710: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.061475: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.155947: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-02 02:51:16.157586: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: /home/zhongruima/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc/bin/ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-08-02 02:51:16.280038: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.308718: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.336578: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.358772: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.382690: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-08-02 02:51:16.407041: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5219/5968 [=========================>....] - ETA: 6s - loss: 0.4231 - accuracy: 0.8636"
     ]
    }
   ],
   "source": [
    "# ATL-SST\n",
    "def main_pipeline(WS, k, Num_Classes, CRDHSI, GT, Tr, TrC, Va, VaC, Te, TeC, num_iterations, query_percentage, epochs, batch_size):\n",
    "    model = MambaViT(WS, k, Num_Classes)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Initialize a dictionary to store sample counts\n",
    "    sample_counts = {\n",
    "        \"initial_training_samples\": Tr.shape[0],\n",
    "        \"initial_Train_class_counts\": count_class_samples(TrC, Num_Classes),\n",
    "        \"initial_validation_samples\": Va.shape[0],\n",
    "        \"initial_Val_class_counts\": count_class_samples(VaC, Num_Classes),\n",
    "        \"Test_samples\": Te.shape[0],\n",
    "        \"class_Test_counts\": count_class_samples(TeC, Num_Classes),\n",
    "        \"iterations\": []\n",
    "    }\n",
    "    # Initial training\n",
    "    start_time = time.time()\n",
    "    model.fit(x=Tr, y=TrC, batch_size=batch_size, epochs=epochs, validation_data=(Va, VaC))\n",
    "    training_time = time.time() - start_time\n",
    "    # Compute FLOPs for the initial model\n",
    "    initial_flops = compute_flops(model)\n",
    "    # Record initial counts\n",
    "    sample_counts[\"iterations\"].append({\n",
    "        \"iteration\": 1,\n",
    "        \"training_samples\": Tr.shape[0],\n",
    "        \"class_train_counts\": count_class_samples(TrC, Num_Classes),\n",
    "        \"validation_samples\": Va.shape[0],\n",
    "        \"class_Val_counts\": count_class_samples(VaC, Num_Classes), \n",
    "        \"Test_samples\": Te.shape[0],\n",
    "        \"class_Test_counts\": count_class_samples(TeC, Num_Classes)\n",
    "    })\n",
    "    \n",
    "    ## Test Phase \n",
    "    start = time.time()\n",
    "    Te_Pre = model.predict(Te)\n",
    "    end = time.time()\n",
    "    Te_Time = end - start\n",
    "    trainable_parameters = model.count_params()\n",
    "    classification, Confusion, OA, Per_Class, AA, Kappa = ClassificationReports(TeC, Te_Pre, target_names)\n",
    "    file_name = f\"1_{HSID}_{trRatio}_{WS}_Classification_Report.csv\"\n",
    "    CSV_file = CSVResults(file_name, classification, Confusion, trainable_parameters, \n",
    "                          initial_flops, training_time, Te_Time, Kappa, OA, AA, Per_Class)\n",
    "\n",
    "    # Ground Truths for inital Training\n",
    "    outputs = GT_Plot(CRDHSI, GT, model, WS, k)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(outputs, cmap='nipy_spectral')\n",
    "    plt.axis('off')\n",
    "    file_name = f\"1_{HSID}_{trRatio}_{WS}_Ground_Truths.png\"\n",
    "    plt.savefig(file_name, dpi=500, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Active Learning Iterations\n",
    "    for i in range(1, num_iterations+1):\n",
    "        print(f\"Active Learning Iteration {i}/{num_iterations}\")\n",
    "        # Query new samples from the pool using uncertainty sampling\n",
    "        query_size = int(query_percentage * Va.shape[0])\n",
    "        query_indices = hybrid_query_strategy(model, Va, query_size)\n",
    "        queried_X = Va[query_indices]\n",
    "        queried_y = VaC[query_indices]\n",
    "        \n",
    "        # Add queried samples back to the training set\n",
    "        Tr = np.concatenate([Tr, queried_X], axis=0)\n",
    "        TrC = np.concatenate([TrC, queried_y], axis=0)\n",
    "        # Remove queried samples from the pool\n",
    "        Va = np.delete(Va, query_indices, axis=0)\n",
    "        VaC = np.delete(VaC, query_indices, axis=0)\n",
    "        # Record counts after querying\n",
    "        sample_counts[\"iterations\"].append({\n",
    "            \"iteration\": i+1,\n",
    "            \"training_samples\": Tr.shape[0],\n",
    "            \"class_train_counts\": count_class_samples(TrC, Num_Classes),\n",
    "            \"validation_samples\": Va.shape[0],\n",
    "            \"class_Val_counts\": count_class_samples(VaC, Num_Classes)\n",
    "        })\n",
    "        # freeze feature learning layers and fine-tune classification layers\n",
    "        for layer in model.layers[:-3]:\n",
    "            layer.trainable = False\n",
    "        # Fine-tuning the model\n",
    "        fine_tune_start_time = time.time()\n",
    "        history = model.fit(x=Tr, y=TrC, batch_size=batch_size, epochs=epochs, validation_data=(Va, VaC))\n",
    "        fine_tune_time = time.time() - fine_tune_start_time\n",
    "        # Compute FLOPs for the Fine_tuned model\n",
    "        Fine_tuned_flops = compute_flops(model)\n",
    "        Finetuned_parameters = model.count_params()\n",
    "        ## Test Phase \n",
    "        start = time.time()\n",
    "        Te_Pre = model.predict(Te)\n",
    "        end = time.time()\n",
    "        Te_Time = end - start\n",
    "        ## Classification Report for Test Model\n",
    "        classification, Confusion, OA, Per_Class, AA, Kappa = ClassificationReports(TeC, Te_Pre, target_names)\n",
    "        file_name = f\"{i+1}_{HSID}_{trRatio}_{WS}_Classification_Report.csv\"\n",
    "        CSV_file = CSVResults(file_name, classification, Confusion, Finetuned_parameters, \n",
    "                          Fine_tuned_flops, fine_tune_time, Te_Time, Kappa, OA, AA, Per_Class)\n",
    "        # Ground Truths\n",
    "        outputs = GT_Plot(CRDHSI, GT, model, WS, k)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(outputs, cmap='nipy_spectral')\n",
    "        plt.axis('off')\n",
    "        file_name = f\"{i+1}_{HSID}_{trRatio}_{WS}_Ground_Truths.png\"\n",
    "        plt.savefig(file_name, dpi=500, format='png', bbox_inches='tight', pad_inches=0)\n",
    "        # Save the model during the last iteration\n",
    "        if i == num_iterations:\n",
    "            model.save(f\"{HSID}_trained_ssvit_model.tf\")\n",
    "            print(f\"Model saved at iteration {i} as 'final_model_iteration.tf'\")\n",
    "\n",
    "    # Save sample counts to a file\n",
    "    sample_counts_filename = f\"{HSID}_{trRatio}_{WS}_sample_counts.json\"\n",
    "    with open(sample_counts_filename, 'w') as f:\n",
    "        json.dump(sample_counts, f, indent=4)    \n",
    "    # Extract training and validation metrics\n",
    "    accuracy = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "        # Check if validation metrics exist\n",
    "    val_accuracy = history.history.get('val_accuracy', [])\n",
    "    val_loss = history.history.get('val_loss', [])\n",
    "    # Create a DataFrame\n",
    "    history_df = pd.DataFrame({\n",
    "        'Epoch': range(1, len(accuracy) + 1),\n",
    "        'Training Accuracy': accuracy,\n",
    "        'Training Loss': loss,\n",
    "        'Validation Accuracy': val_accuracy if val_accuracy else ['N/A'] * len(accuracy),\n",
    "        'Validation Loss': val_loss if val_loss else ['N/A'] * len(accuracy)\n",
    "    })\n",
    "    # Save to CSV\n",
    "    history_df.to_csv(f\"{HSID}_training_history.csv\", index=False)\n",
    "    return model\n",
    "# Main\n",
    "model = main_pipeline(WS, k, Num_Classes, CRDHSI, GT, Tr, TrC, Va, VaC, Te, TeC, num_iterations=5, \n",
    "                      query_percentage=0.02, epochs=epochs, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
